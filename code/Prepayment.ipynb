{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac25676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Path where your massive time files are stored\n",
    "DATA_DIR = \"C:/Users/hansh/OneDrive/Desktop/OBITS Lab/MBS Simulate/data/Freddie data/\"\n",
    "PERF_FILE = DATA_DIR + \"combined_performance_history_2017_2020.csv\"\n",
    "ORIG_FILE = DATA_DIR + \"combined_sampled_mortgages_2017_2020.csv\"\n",
    "OUTPUT_FILE = DATA_DIR + \"mortgage_survival_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6b7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Origination Data...\n",
      "2. Processing Performance Data (Grouping by Loan)...\n",
      "   Calculating Events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_19328\\3388883756.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = df_perf.groupby(\"LOAN_SEQUENCE_NUMBER\").apply(get_event_status)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Merging Covariates...\n",
      "Done! Saved 80000 loans to C:/Users/hansh/OneDrive/Desktop/OBITS Lab/MBS Simulate/data/Freddie data/mortgage_survival_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Updated Columns based on your sample (33 Cols)\n",
    "TIME_COLS = [\n",
    "    \"LOAN_SEQUENCE_NUMBER\", \"MONTHLY_REPORTING_PERIOD\", \"CURRENT_ACTUAL_UPB\", \n",
    "    \"CURRENT_LOAN_DELINQUENCY_STATUS\", \"LOAN_AGE\", \"REMAINING_MONTHS_TO_LEGAL_MATURITY\", \n",
    "    \"REPURCHASE_FLAG\", \"MODIFICATION_FLAG\", \"ZERO_BALANCE_CODE\", \n",
    "    \"ZERO_BALANCE_EFFECTIVE_DATE\", \"CURRENT_INTEREST_RATE\", \"CURRENT_DEFERRED_UPB\", \n",
    "    \"DUE_DATE_OF_LAST_PAID_INSTALLMENT\", \"MI_RECOVERIES\", \"NET_SALES_PROCEEDS\", \n",
    "    \"NON_MI_RECOVERIES\", \"EXPENSES\", \"LEGAL_COSTS\", \"MAINTENANCE_AND_PRESERVATION_COSTS\", \n",
    "    \"TAXES_AND_INSURANCE\", \"MISC_EXPENSES\", \"ACTUAL_LOSS_CALCULATION\", \"MODIFICATION_COST\", \n",
    "    \"STEP_MODIFICATION_FLAG\", \"DEFERRED_PAYMENT_PLAN\", \"ESTIMATED_LOAN_TO_VALUE\", \n",
    "    \"ZERO_BALANCE_REMOVAL_UPB\", \"DELINQUENT_ACCRUED_INTEREST\", \"DELINQUENCY_DUE_TO_DISASTER\", \n",
    "    \"BORROWER_ASSISTANCE_STATUS_CODE\", \"CURRENT_MONTH_MODIFICATION_COST\", \"INTEREST_BEARING_UPB\", \n",
    "    \"SOURCE_QUARTER\"\n",
    "]\n",
    "\n",
    "def get_event_status(group):\n",
    "    \"\"\"\n",
    "    Determines the terminal state of a loan.\n",
    "    Returns: (Duration, Event_Type)\n",
    "    Event 0: Censored (Active)\n",
    "    Event 1: Prepaid\n",
    "    Event 2: Default\n",
    "    \"\"\"\n",
    "    # 1. Sort by Age to ensure chronological order\n",
    "    group = group.sort_values(\"LOAN_AGE\")\n",
    "    last_row = group.iloc[-1]\n",
    "    \n",
    "    # 2. Check Terminal State (Zero Balance Code)\n",
    "    # Handle both string '01' and float 1.0\n",
    "    zbc = last_row[\"ZERO_BALANCE_CODE\"]\n",
    "    \n",
    "    try:\n",
    "        zbc_float = float(zbc)\n",
    "    except (ValueError, TypeError):\n",
    "        zbc_float = 0.0\n",
    "\n",
    "    # Logic: Prepay (Code 1)\n",
    "    if zbc_float == 1.0:\n",
    "        return last_row[\"LOAN_AGE\"], 1\n",
    "    \n",
    "    # Logic: Default (Codes 3, 6, 9)\n",
    "    if zbc_float in [3.0, 6.0, 9.0]:\n",
    "        return last_row[\"LOAN_AGE\"], 2\n",
    "        \n",
    "    # 3. Check Delinquency History (Technical Default)\n",
    "    # If a loan ever hits 6+ months delinquent (D180), we treat it as a default event\n",
    "    # even if it cures later (Competing Risk logic).\n",
    "    # Convert 'R', 'XX', etc to NaN -> 0\n",
    "    delinq_vals = pd.to_numeric(group[\"CURRENT_LOAN_DELINQUENCY_STATUS\"], errors='coerce').fillna(0)\n",
    "    \n",
    "    if (delinq_vals >= 6).any():\n",
    "        # Duration is the FIRST time it hit D6\n",
    "        first_def_idx = (delinq_vals >= 6).idxmax()\n",
    "        duration = group.loc[first_def_idx, \"LOAN_AGE\"]\n",
    "        return duration, 2\n",
    "\n",
    "    # 4. Censored (Still Active)\n",
    "    return last_row[\"LOAN_AGE\"], 0\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Origination Data...\")\n",
    "    df_orig = pd.read_csv(ORIG_FILE, sep='|', low_memory=False)\n",
    "    \n",
    "    print(\"2. Processing Performance Data (Grouping by Loan)...\")\n",
    "    # Reading the large combined file. If this is too large for RAM, use chunking (previous method).\n",
    "    # Assuming the 6000-loan merged file fits in memory.\n",
    "    df_perf = pd.read_csv(PERF_FILE, sep='|', names=TIME_COLS, header=0, low_memory=False)\n",
    "    \n",
    "    # Filter to ensure we only process the IDs we have originations for\n",
    "    target_ids = set(df_orig['LOAN_SEQUENCE_NUMBER'])\n",
    "    df_perf = df_perf[df_perf['LOAN_SEQUENCE_NUMBER'].isin(target_ids)]\n",
    "    \n",
    "    # Apply Logic\n",
    "    print(\"   Calculating Events...\")\n",
    "    events = df_perf.groupby(\"LOAN_SEQUENCE_NUMBER\").apply(get_event_status)\n",
    "    \n",
    "    # Structure into DataFrame\n",
    "    df_events = pd.DataFrame(events.tolist(), index=events.index, columns=['DURATION', 'EVENT'])\n",
    "    \n",
    "    # Merge Covariates\n",
    "    print(\"3. Merging Covariates...\")\n",
    "    final_df = df_orig.merge(df_events, left_on='LOAN_SEQUENCE_NUMBER', right_index=True)\n",
    "    \n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Done! Saved {len(final_df)} loans to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c461b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_11268\\3984040502.py:33: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR + \"mortgage_survival_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Processed data saved to C:/Users/hansh/OneDrive/Desktop/OBITS Lab/MBS Simulate/data/Freddie data/processed_mortgage_data_for_modeling.csv\n",
      "  LOAN_SEQUENCE_NUMBER  DURATION  EVENT  RATE_INCENTIVE  BURNOUT_PROXY\n",
      "0         F17Q10141137        98      0           1.250         100.25\n",
      "1         F17Q10000777        51      1           1.190          26.28\n",
      "2         F17Q10240479        98      0           0.000          26.70\n",
      "3         F17Q10166939        60      1           0.490          37.70\n",
      "4         F17Q10287022        35      1           0.905          11.40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define Market Rates (Full dictionary from previous context)\n",
    "MARKET_RATES = {\n",
    "    # 2017\n",
    "    201701: 4.15, 201702: 4.17, 201703: 4.20, 201704: 4.05, 201705: 4.01, 201706: 3.90,\n",
    "    201707: 3.97, 201708: 3.88, 201709: 3.81, 201710: 3.90, 201711: 3.92, 201712: 3.95,\n",
    "    # 2018\n",
    "    201801: 4.03, 201802: 4.33, 201803: 4.44, 201804: 4.47, 201805: 4.59, 201806: 4.57,\n",
    "    201807: 4.53, 201808: 4.55, 201809: 4.63, 201810: 4.83, 201811: 4.87, 201812: 4.64,\n",
    "    # 2019\n",
    "    201901: 4.46, 201902: 4.37, 201903: 4.27, 201904: 4.14, 201905: 4.07, 201906: 3.80,\n",
    "    201907: 3.77, 201908: 3.62, 201909: 3.61, 201910: 3.69, 201911: 3.70, 201912: 3.72,\n",
    "    # 2020\n",
    "    202001: 3.62, 202002: 3.47, 202003: 3.45, 202004: 3.31, 202005: 3.23, 202006: 3.16,\n",
    "    202007: 3.02, 202008: 2.94, 202009: 2.89, 202010: 2.83, 202011: 2.77, 202012: 2.68,\n",
    "    # 2021\n",
    "    202101: 2.74, 202102: 2.81, 202103: 3.08, 202104: 3.06, 202105: 2.96, 202106: 2.98,\n",
    "    202107: 2.87, 202108: 2.84, 202109: 2.90, 202110: 3.07, 202111: 3.07, 202112: 3.10,\n",
    "    # 2022\n",
    "    202201: 3.45, 202202: 3.76, 202203: 4.17, 202204: 4.98, 202205: 5.23, 202206: 5.52,\n",
    "    202207: 5.41, 202208: 5.22, 202209: 6.11, 202210: 6.90, 202211: 6.76, 202212: 6.35,\n",
    "    # 2023\n",
    "    202301: 6.25, 202302: 6.30, 202303: 6.54, 202304: 6.34, 202305: 6.43, 202306: 6.71,\n",
    "    202307: 6.84, 202308: 7.07, 202309: 7.20, 202310: 7.62, 202311: 7.44, 202312: 6.82,\n",
    "    # 2024\n",
    "    202401: 6.64, 202402: 6.78, 202403: 6.82, 202404: 6.99, 202405: 7.06, 202406: 6.92,\n",
    "    202407: 6.82, 202408: 6.50, 202409: 6.18, 202410: 6.43, 202411: 6.81, 202412: 6.72\n",
    "}\n",
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv(DATA_DIR + \"mortgage_survival_dataset.csv\")\n",
    "\n",
    "# 3. Feature Engineering Function\n",
    "def get_incentive_metrics(row):\n",
    "    try:\n",
    "        start_date = int(row['FIRST_PAYMENT_DATE'])\n",
    "        duration = int(row['DURATION'])\n",
    "        note_rate = row['ORIGINAL_INTEREST_RATE']\n",
    "        \n",
    "        start_year = start_date // 100\n",
    "        start_month = start_date % 100\n",
    "        \n",
    "        cumulative_incentive = 0.0\n",
    "        current_incentive = 0.0\n",
    "        \n",
    "        # Limit loop to avoid hanging on bad data, though DURATION should be reasonable\n",
    "        duration = min(duration, 360) \n",
    "        \n",
    "        for i in range(duration):\n",
    "            total_months = start_month + i - 1\n",
    "            curr_year = start_year + (total_months // 12)\n",
    "            curr_month = (total_months % 12)\n",
    "            if curr_month == 0:\n",
    "                curr_month = 12\n",
    "                curr_year -= 1\n",
    "            \n",
    "            yyyymm = (curr_year * 100) + curr_month\n",
    "            market_rate = MARKET_RATES.get(yyyymm, 4.0)\n",
    "            \n",
    "            incentive = note_rate - market_rate\n",
    "            \n",
    "            if incentive > 0:\n",
    "                cumulative_incentive += incentive\n",
    "            \n",
    "            if i == duration - 1:\n",
    "                current_incentive = incentive\n",
    "                \n",
    "        return pd.Series([current_incentive, cumulative_incentive])\n",
    "    except:\n",
    "        return pd.Series([0.0, 0.0])\n",
    "\n",
    "# 4. Apply Feature Engineering\n",
    "print(\"Processing data...\")\n",
    "metrics = df.apply(get_incentive_metrics, axis=1)\n",
    "df[['RATE_INCENTIVE', 'BURNOUT_PROXY']] = metrics\n",
    "\n",
    "# 5. Save Processed Data for User\n",
    "output_filename = DATA_DIR + \"processed_mortgage_data_for_modeling.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_filename}\")\n",
    "print(df[['LOAN_SEQUENCE_NUMBER', 'DURATION', 'EVENT', 'RATE_INCENTIVE', 'BURNOUT_PROXY']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb652a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Processed Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_11268\\1429454192.py:20: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Setting up Prepayment Target...\n",
      "   Modeling Universe: 80000 loans\n",
      "\n",
      "2. Splitting Data (25% Hold-out)...\n",
      "\n",
      "3. Training Cox Proportional Hazard Model...\n",
      "   -> Cox Model C-Index (Test): 0.8015\n",
      "\n",
      "   Cox Coefficients:\n",
      "covariate\n",
      "RATE_INCENTIVE                   0.151122\n",
      "BURNOUT_PROXY                   -0.090219\n",
      "CREDIT_SCORE                     0.000161\n",
      "ORIGINAL_LTV                    -0.000931\n",
      "ORIGINAL_DEBT_TO_INCOME_RATIO   -0.000444\n",
      "ORIGINAL_INTEREST_RATE           2.304980\n",
      "Name: coef, dtype: float64\n",
      "\n",
      "4. Training Random Survival Forest (RSF)...\n",
      "   -> RSF Model C-Index (Test): 0.9064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_11268\\1429454192.py:97: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  preds = np.row_stack([fn(times) for fn in surv_probs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> RSF Integrated Brier Score: 0.0513 (Lower is better)\n",
      "\n",
      "========================================\n",
      "FINAL MODEL COMPARISON\n",
      "========================================\n",
      "Metric               | Cox Model  | RSF (ML)  \n",
      "----------------------------------------\n",
      "C-Index (Ranking)    | 0.8015     | 0.9064\n",
      "----------------------------------------\n",
      "\n",
      "Winner: Random Survival Forest\n",
      "Reasoning: The Prepayment function is likely non-linear (S-Curve).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You need to install these: pip install lifelines scikit-survival\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_FILE = DATA_DIR + \"processed_mortgage_data_for_modeling.csv\"\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Processed Data...\")\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    \n",
    "    # Define Predictors and Target\n",
    "    # We use our new features: RATE_INCENTIVE and BURNOUT_PROXY\n",
    "    features = ['RATE_INCENTIVE', 'BURNOUT_PROXY', 'CREDIT_SCORE', 'ORIGINAL_LTV', \n",
    "                'ORIGINAL_DEBT_TO_INCOME_RATIO', 'ORIGINAL_INTEREST_RATE']\n",
    "    \n",
    "    target_duration = 'DURATION'\n",
    "    target_event = 'EVENT' # 0=Censored, 1=Prepay, 2=Default\n",
    "    \n",
    "    # Filter: Analysis of Prepayment Risk Only\n",
    "    # Treat Defaults (2) as Censored (0)\n",
    "    print(\"   Setting up Prepayment Target...\")\n",
    "    df['IS_PREPAID'] = df[target_event].apply(lambda x: 1 if x == 1 else 0)\n",
    "    \n",
    "    # Drop rows with missing values to prevent model errors\n",
    "    df_clean = df[features + [target_duration, 'IS_PREPAID']].dropna()\n",
    "    print(f\"   Modeling Universe: {len(df_clean)} loans\")\n",
    "\n",
    "    # 2. Split Data (Train / Test)\n",
    "    print(f\"\\n2. Splitting Data ({int(TEST_SIZE*100)}% Hold-out)...\")\n",
    "    train, test = train_test_split(df_clean, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # MODEL A: COX PROPORTIONAL HAZARD\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n3. Training Cox Proportional Hazard Model...\")\n",
    "    cph = CoxPHFitter()\n",
    "    try:\n",
    "        cph.fit(train, duration_col=target_duration, event_col='IS_PREPAID')\n",
    "        \n",
    "        # Evaluate C-Index\n",
    "        cox_pred = cph.predict_partial_hazard(test)\n",
    "        cox_c_index = concordance_index(test[target_duration], -cox_pred, test['IS_PREPAID'])\n",
    "        print(f\"   -> Cox Model C-Index (Test): {cox_c_index:.4f}\")\n",
    "        \n",
    "        # Print Coefficients to verify Incentive/Burnout logic\n",
    "        print(\"\\n   Cox Coefficients:\")\n",
    "        print(cph.params_)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Cox Error: {e}\")\n",
    "        cox_c_index = 0\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # MODEL B: RANDOM SURVIVAL FOREST (ML)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n4. Training Random Survival Forest (RSF)...\")\n",
    "    \n",
    "    # Format data for Scikit-Survival\n",
    "    X_train = train[features]\n",
    "    X_test = test[features]\n",
    "    \n",
    "    # Create structured Target array (Boolean Event, Time)\n",
    "    y_train_surv = Surv.from_dataframe(\"IS_PREPAID\", target_duration, train)\n",
    "    y_test_surv = Surv.from_dataframe(\"IS_PREPAID\", target_duration, test)\n",
    "    \n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=100, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=15, \n",
    "        n_jobs=-1, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    rsf.fit(X_train, y_train_surv)\n",
    "    \n",
    "    # Evaluate C-Index\n",
    "    rsf_c_index = rsf.score(X_test, y_test_surv)\n",
    "    print(f\"   -> RSF Model C-Index (Test): {rsf_c_index:.4f}\")\n",
    "    \n",
    "    # Evaluate Integrated Brier Score (IBS) - Accuracy Check\n",
    "    # We check accuracy at specific time points (e.g. 12, 24, 36 months)\n",
    "    try:\n",
    "        times = np.quantile(test[target_duration][test['IS_PREPAID']==1], np.linspace(0.1, 0.9, 10))\n",
    "        surv_probs = rsf.predict_survival_function(X_test)\n",
    "        \n",
    "        # Matrix of probabilities for IBS\n",
    "        preds = np.row_stack([fn(times) for fn in surv_probs])\n",
    "        ibs_score = integrated_brier_score(y_train_surv, y_test_surv, preds, times)\n",
    "        print(f\"   -> RSF Integrated Brier Score: {ibs_score:.4f} (Lower is better)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   (IBS Calculation skipped: {e})\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # COMPARISON\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL MODEL COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"{'Metric':<20} | {'Cox Model':<10} | {'RSF (ML)':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'C-Index (Ranking)':<20} | {cox_c_index:.4f}     | {rsf_c_index:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if rsf_c_index > cox_c_index:\n",
    "        print(\"\\nWinner: Random Survival Forest\")\n",
    "        print(\"Reasoning: The Prepayment function is likely non-linear (S-Curve).\")\n",
    "    else:\n",
    "        print(\"\\nWinner: Cox Proportional Hazard\")\n",
    "        print(\"Reasoning: The relationship is linear and Incentive-driven.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e6ce1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models to disk...\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# ... (After fitting cph and rsf) ...\n",
    "\n",
    "print(\"Saving models to disk...\")\n",
    "\n",
    "# 1. Save Cox Model\n",
    "# Note: Lifelines objects pickle well, but ensure you use the same library version when loading.\n",
    "joblib.dump(cph, DATA_DIR + \"cox_prepayment_model.pkl\")\n",
    "\n",
    "# 2. Save Random Survival Forest\n",
    "# Scikit-survival objects are scikit-learn compatible.\n",
    "joblib.dump(rsf, DATA_DIR + \"rsf_prepayment_model.pkl\")\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of MBS Pool Prime Candidates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "file_path = DATA_DIR + \"combined_sampled_mortgages_2017_2020.csv\"\n",
    "df = pd.read_csv(file_path, sep='|', low_memory=False)\n",
    "\n",
    "# 2. Data Cleaning & Standardization\n",
    "# Convert numeric columns, handling errors\n",
    "cols_to_clean = ['CREDIT_SCORE', 'ORIGINAL_UPB', 'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO', 'ORIGINAL_INTEREST_RATE']\n",
    "for col in cols_to_clean:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Handle specific valid ranges/missing codes\n",
    "# Freddie Mac: 9999 for Credit Score is missing. 999 for DTI is missing.\n",
    "df['CREDIT_SCORE'] = df['CREDIT_SCORE'].apply(lambda x: x if 300 <= x <= 850 else np.nan)\n",
    "df['ORIGINAL_DEBT_TO_INCOME_RATIO'] = df['ORIGINAL_DEBT_TO_INCOME_RATIO'].apply(lambda x: x if 0 <= x <= 100 else np.nan)\n",
    "\n",
    "# Drop rows with critical missing data for underwriting\n",
    "df_clean = df.dropna(subset=['CREDIT_SCORE', 'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO', 'ORIGINAL_UPB'])\n",
    "\n",
    "# 3. Apply \"Prime MBS\" Eligibility Criteria (Quality)\n",
    "# Criteria: High FICO, Good Equity (LTV), Ability to Pay (DTI), Owner Occupied\n",
    "# Note: These are stricter than standard Agency to create a \"High Quality\" pool.\n",
    "prime_mask = (\n",
    "    (df_clean['CREDIT_SCORE'] >= 720) &       # Prime Credit\n",
    "    (df_clean['ORIGINAL_LTV'] <= 80) &        # Significant Equity (<=80% usually avoids MI)\n",
    "    (df_clean['ORIGINAL_DEBT_TO_INCOME_RATIO'] <= 43) & # QM Safe Harbor standard\n",
    "    (df_clean['OCCUPANCY_STATUS'] == 'P') &   # Principal Residence only (lower default risk)\n",
    "    (df_clean['PROPERTY_TYPE'].isin(['SF', 'PU'])) # Single Family or PUD (avoid condos/co-ops for uniformity if desired, though condos are fine)\n",
    ")\n",
    "\n",
    "eligible_pool = df_clean[prime_mask]\n",
    "\n",
    "print(f\"Original Pool Size: {len(df)}\")\n",
    "print(f\"Cleaned Pool Size: {len(df_clean)}\")\n",
    "print(f\"Eligible 'Prime' Candidates: {len(eligible_pool)}\")\n",
    "\n",
    "# 4. Selection & Diversification (Geographic)\n",
    "# We want 6000 loans.\n",
    "# Strategy: Stratified Sampling by State to ensure geographic representativeness of the Prime universe,\n",
    "# but we might want to cap exposure to any single state (e.g., CA/TX/FL) if they are too dominant.\n",
    "\n",
    "target_size = 6000\n",
    "\n",
    "if len(eligible_pool) < target_size:\n",
    "    print(\"Warning: Not enough loans met the strict 'Prime' criteria. Relaxing criteria...\")\n",
    "    # Relax logic would go here, but with 80k rows, we likely have enough.\n",
    "    # Fallback: Just take top N sorted by Credit Score\n",
    "    selected_pool = eligible_pool.sort_values('CREDIT_SCORE', ascending=False).head(target_size)\n",
    "else:\n",
    "    # Check State Concentrations in Eligible Pool\n",
    "    state_counts = eligible_pool['PROPERTY_STATE'].value_counts(normalize=True)\n",
    "    \n",
    "    # Simple random sample from eligible pool generally preserves distribution.\n",
    "    # Let's do a random sample first.\n",
    "    selected_pool = eligible_pool.sample(n=target_size, random_state=42)\n",
    "\n",
    "# 5. Analysis of the Selected Pool vs Original\n",
    "def get_stats(d, name):\n",
    "    return pd.Series({\n",
    "        'Count': len(d),\n",
    "        'Total UPB ($M)': d['ORIGINAL_UPB'].sum() / 1e6,\n",
    "        'WA FICO': np.average(d['CREDIT_SCORE'], weights=d['ORIGINAL_UPB']),\n",
    "        'WA LTV': np.average(d['ORIGINAL_LTV'], weights=d['ORIGINAL_UPB']),\n",
    "        'WA DTI': np.average(d['ORIGINAL_DEBT_TO_INCOME_RATIO'], weights=d['ORIGINAL_UPB']),\n",
    "        'WA Rate': np.average(d['ORIGINAL_INTEREST_RATE'], weights=d['ORIGINAL_UPB']),\n",
    "        'Top State %': d['PROPERTY_STATE'].value_counts(normalize=True).iloc[0] * 100,\n",
    "        'Top State': d['PROPERTY_STATE'].value_counts(normalize=True).index[0]\n",
    "    }, name=name)\n",
    "\n",
    "stats_orig = get_stats(df_clean, \"Original (Cleaned)\")\n",
    "stats_pool = get_stats(selected_pool, \"Selected MBS Pool\")\n",
    "\n",
    "comparison = pd.concat([stats_orig, stats_pool], axis=1)\n",
    "\n",
    "# Geographic Diversification Check (Top 5 States)\n",
    "top_states_pool = selected_pool['PROPERTY_STATE'].value_counts(normalize=True).head(5) * 100\n",
    "\n",
    "print(\"\\n--- Comparative Statistics ---\")\n",
    "print(comparison)\n",
    "print(\"\\n--- Geographic Concentration (Selected Pool) ---\")\n",
    "print(top_states_pool)\n",
    "\n",
    "# 6. Save Result\n",
    "selected_pool.to_csv(\"selected_6000_prime_mbs_pool.csv\", sep='|', index=False)\n",
    "print(\"\\nSaved to 'selected_6000_prime_mbs_pool.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9156e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NEW_DATA_FILE = \"selected_mbs_pool_6000.csv\"  # The pool we selected earlier\n",
    "MARKET_RATE_NOW = 6.72  # Current market rate (Dec 2024) for incentive calc\n",
    "OUTPUT_SCORES = \"scored_mbs_pool.csv\"\n",
    "\n",
    "# 1. Load Models\n",
    "print(\"1. Loading Models...\")\n",
    "cph = joblib.load(\"cox_prepayment_model.pkl\")\n",
    "rsf = joblib.load(\"rsf_prepayment_model.pkl\")\n",
    "\n",
    "# 2. Load New Data\n",
    "print(\"2. Loading New Portfolio...\")\n",
    "df = pd.read_csv(NEW_DATA_FILE, sep='|', low_memory=False)\n",
    "\n",
    "# 3. Feature Engineering (Generate Predictors)\n",
    "print(\"3. Generating Predictors...\")\n",
    "\n",
    "# A. Rate Incentive (Simple \"Spot\" Calculation for new loans)\n",
    "# Incentive = Note Rate - Current Market Rate\n",
    "df['RATE_INCENTIVE'] = df['ORIGINAL_INTEREST_RATE'] - MARKET_RATE_NOW\n",
    "\n",
    "# B. Burnout Proxy\n",
    "# For new loans (Age=0), Burnout is 0.\n",
    "# For seasoned loans, you would need their history.\n",
    "# Assuming these are seasoned loans from our 2017-2020 set:\n",
    "# (We use a simplified logic here: If Incentive < 0, Burnout stays 0. If > 0, it accumulates).\n",
    "# For scoring a static file without full history, we might approximate or set to 0 if unknown.\n",
    "df['BURNOUT_PROXY'] = 0.0 # Placeholder if full history unavailable\n",
    "\n",
    "# Ensure columns match training features EXACTLY\n",
    "features = ['RATE_INCENTIVE', 'BURNOUT_PROXY', 'CREDIT_SCORE', \n",
    "            'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO', 'ORIGINAL_INTEREST_RATE']\n",
    "\n",
    "# Handle missing data (e.g., fillna with training means)\n",
    "df_score = df[features].fillna(0)\n",
    "\n",
    "# 4. Scoring (Prediction)\n",
    "print(\"4. Scoring Loans...\")\n",
    "\n",
    "# --- SCORE A: COX HAZARD (Relative Risk) ---\n",
    "# Returns \"partial hazard\". Exp(partial) = Multiplier vs Baseline.\n",
    "# e.g., 1.5 means \"50% more likely to prepay than average\".\n",
    "df['COX_RISK_SCORE'] = cph.predict_partial_hazard(df_score)\n",
    "\n",
    "# --- SCORE B: RSF SURVIVAL PROBABILITY (12-Month) ---\n",
    "# \"What is the probability this loan survives (does NOT prepay) for the next 12 months?\"\n",
    "# We need to predict survival function, then look up t=12.\n",
    "surv_funcs = rsf.predict_survival_function(df_score)\n",
    "\n",
    "# Extract probability at t=12 months\n",
    "prob_survival_12m = []\n",
    "for fn in surv_funcs:\n",
    "    # fn is a step function. We evaluate it at x=12.\n",
    "    # Note: If 12 is beyond the training duration (unlikely), it returns last known value.\n",
    "    try:\n",
    "        p = fn(12) \n",
    "    except:\n",
    "        p = 0.0 # Fallback\n",
    "    prob_survival_12m.append(p)\n",
    "\n",
    "df['RSF_SURV_PROB_12M'] = prob_survival_12m\n",
    "\n",
    "# 5. Ranking & Strategy\n",
    "# Define \"Fast Prepay\" as High Cox Score OR Low RSF Survival Prob\n",
    "df['PREPAY_RANK'] = df['COX_RISK_SCORE'].rank(ascending=False)\n",
    "\n",
    "# 6. Save\n",
    "print(f\"5. Saving Scores to {OUTPUT_SCORES}...\")\n",
    "output_cols = ['LOAN_SEQUENCE_NUMBER', 'ORIGINAL_INTEREST_RATE', 'RATE_INCENTIVE', \n",
    "               'COX_RISK_SCORE', 'RSF_SURV_PROB_12M', 'PREPAY_RANK']\n",
    "df[output_cols].to_csv(OUTPUT_SCORES, index=False)\n",
    "\n",
    "print(\"\\n--- TOP 5 FASTEST PREPAY CANDIDATES ---\")\n",
    "print(df[output_cols].sort_values(by='COX_RISK_SCORE', ascending=False).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
