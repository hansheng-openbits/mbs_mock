{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84405f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_14356\\2965294916.py:5: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Distribution:\n",
      "EVENT\n",
      "1    51354\n",
      "0    27066\n",
      "2     1580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Correlation with Default:\n",
      "IS_DEFAULT                       1.000000\n",
      "CREDIT_SCORE                    -0.041533\n",
      "ORIGINAL_LTV                     0.039737\n",
      "ORIGINAL_DEBT_TO_INCOME_RATIO    0.034488\n",
      "Name: IS_DEFAULT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"C:/Users/hansh/OneDrive/Desktop/OBITS Lab/MBS Simulate/data/Freddie data/\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n",
    "\n",
    "# Check distribution of EVENT\n",
    "# 0 = Censored, 1 = Prepay, 2 = Default\n",
    "event_counts = df['EVENT'].value_counts()\n",
    "print(\"Event Distribution:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Check correlation of Default with Credit Score and LTV\n",
    "df['IS_DEFAULT'] = df['EVENT'].apply(lambda x: 1 if x == 2 else 0)\n",
    "print(\"\\nCorrelation with Default:\")\n",
    "print(df[['IS_DEFAULT', 'CREDIT_SCORE', 'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO']].corr()['IS_DEFAULT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c70c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_14356\\1486499680.py:4: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Counts:\n",
      "EVENT\n",
      "1    51354\n",
      "0    27066\n",
      "2     1580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Loans: 80000\n",
      "Total Defaults: 1580 (1.98%)\n",
      "\n",
      "Correlation with Default:\n",
      "IS_DEFAULT                       1.000000\n",
      "CREDIT_SCORE                    -0.041533\n",
      "ORIGINAL_LTV                     0.039737\n",
      "ORIGINAL_DEBT_TO_INCOME_RATIO    0.034488\n",
      "Name: IS_DEFAULT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n",
    "\n",
    "# Check Event distribution\n",
    "event_counts = df['EVENT'].value_counts()\n",
    "print(\"Event Counts:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Check Default Rate\n",
    "default_count = event_counts.get(2, 0) # Event 2 is Default\n",
    "total_count = len(df)\n",
    "print(f\"\\nTotal Loans: {total_count}\")\n",
    "print(f\"Total Defaults: {default_count} ({default_count/total_count:.2%})\")\n",
    "\n",
    "# Check correlation of FICO/LTV with Default\n",
    "df['IS_DEFAULT'] = (df['EVENT'] == 2).astype(int)\n",
    "print(\"\\nCorrelation with Default:\")\n",
    "print(df[['IS_DEFAULT', 'CREDIT_SCORE', 'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO']].corr()['IS_DEFAULT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef14fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\AppData\\Local\\Temp\\ipykernel_14356\\2709677748.py:5: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Default Features...\n",
      "Default dataset saved to C:/Users/hansh/OneDrive/Desktop/OBITS Lab/MBS Simulate/data/Freddie data/default_modeling_dataset.csv (80000 loans)\n",
      "  LOAN_SEQUENCE_NUMBER  DURATION  IS_DEFAULT  CREDIT_SCORE  ORIGINAL_LTV  \\\n",
      "0         F17Q10141137        98           0           705            75   \n",
      "1         F17Q10000777        51           0           722            95   \n",
      "2         F17Q10240479        98           0           719            56   \n",
      "3         F17Q10166939        60           0           745            78   \n",
      "4         F17Q10287022        35           0           770            80   \n",
      "\n",
      "   ORIGINAL_DEBT_TO_INCOME_RATIO   SATO  FICO_BUCKET  HIGH_LTV_FLAG  \n",
      "0                             32  1.240          2.0            0.0  \n",
      "1                             47  0.050          2.0            1.0  \n",
      "2                             35 -0.010          2.0            0.0  \n",
      "3                             34  0.200          2.0            0.0  \n",
      "4                             40  0.365          1.0            0.0  \n",
      "\n",
      "Default Rate by FICO Bucket (1=Best, 4=Worst):\n",
      "FICO_BUCKET\n",
      "1.0    0.008539\n",
      "2.0    0.024684\n",
      "3.0    0.044868\n",
      "4.0    0.072345\n",
      "Name: IS_DEFAULT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(DATA_DIR + \"processed_mortgage_data_for_modeling.csv\")\n",
    "\n",
    "# 2. Market Rates Dictionary (re-defined for clarity in this context)\n",
    "MARKET_RATES = {\n",
    "    # 2017\n",
    "    201701: 4.15, 201702: 4.17, 201703: 4.20, 201704: 4.05, 201705: 4.01, 201706: 3.90,\n",
    "    201707: 3.97, 201708: 3.88, 201709: 3.81, 201710: 3.90, 201711: 3.92, 201712: 3.95,\n",
    "    # 2018\n",
    "    201801: 4.03, 201802: 4.33, 201803: 4.44, 201804: 4.47, 201805: 4.59, 201806: 4.57,\n",
    "    201807: 4.53, 201808: 4.55, 201809: 4.63, 201810: 4.83, 201811: 4.87, 201812: 4.64,\n",
    "    # 2019\n",
    "    201901: 4.46, 201902: 4.37, 201903: 4.27, 201904: 4.14, 201905: 4.07, 201906: 3.80,\n",
    "    201907: 3.77, 201908: 3.62, 201909: 3.61, 201910: 3.69, 201911: 3.70, 201912: 3.72,\n",
    "    # 2020\n",
    "    202001: 3.62, 202002: 3.47, 202003: 3.45, 202004: 3.31, 202005: 3.23, 202006: 3.16,\n",
    "    202007: 3.02, 202008: 2.94, 202009: 2.89, 202010: 2.83, 202011: 2.77, 202012: 2.68\n",
    "}\n",
    "\n",
    "# 3. Feature Engineering for Default\n",
    "def engineer_default_features(row):\n",
    "    # A. SATO (Spread At Origination)\n",
    "    # Did the bank charge them a premium? (High SATO = Hidden Risk)\n",
    "    try:\n",
    "        orig_date = int(row['FIRST_PAYMENT_DATE'])\n",
    "        # Handle simple quarter/month offsets if needed, but direct lookup is usually fine for first payment\n",
    "        mkt_rate_orig = MARKET_RATES.get(orig_date, 4.0) \n",
    "        sato = row['ORIGINAL_INTEREST_RATE'] - mkt_rate_orig\n",
    "    except:\n",
    "        sato = 0.0\n",
    "\n",
    "    # B. FICO Buckets (Non-linear risk)\n",
    "    fico = row['CREDIT_SCORE']\n",
    "    if fico >= 750: fico_bucket = 1 # Super Prime\n",
    "    elif fico >= 700: fico_bucket = 2 # Prime\n",
    "    elif fico >= 660: fico_bucket = 3 # Near Prime\n",
    "    else: fico_bucket = 4 # Subprime/Risky\n",
    "\n",
    "    # C. Equity Risk (High LTV)\n",
    "    ltv = row['ORIGINAL_LTV']\n",
    "    high_ltv_flag = 1 if ltv > 80 else 0\n",
    "    \n",
    "    return pd.Series([sato, fico_bucket, high_ltv_flag])\n",
    "\n",
    "print(\"Engineering Default Features...\")\n",
    "df[['SATO', 'FICO_BUCKET', 'HIGH_LTV_FLAG']] = df.apply(engineer_default_features, axis=1)\n",
    "\n",
    "# 4. Prepare Final Dataset\n",
    "# Target: EVENT = 2 (Default). We treat Event 1 (Prepay) as Censored (0) for this specific model.\n",
    "df['IS_DEFAULT'] = df['EVENT'].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "# Select Columns\n",
    "cols = ['LOAN_SEQUENCE_NUMBER', 'DURATION', 'IS_DEFAULT', \n",
    "        'CREDIT_SCORE', 'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO', \n",
    "        'SATO', 'FICO_BUCKET', 'HIGH_LTV_FLAG']\n",
    "\n",
    "df_default = df[cols].dropna()\n",
    "\n",
    "# Save\n",
    "output_file = DATA_DIR + \"default_modeling_dataset.csv\"\n",
    "df_default.to_csv(output_file, index=False)\n",
    "print(f\"Default dataset saved to {output_file} ({len(df_default)} loans)\")\n",
    "print(df_default.head())\n",
    "print(\"\\nDefault Rate by FICO Bucket (1=Best, 4=Worst):\")\n",
    "print(df_default.groupby('FICO_BUCKET')['IS_DEFAULT'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8fe8a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Default Data...\n",
      "   Modeling Universe: 80000 loans\n",
      "   Training: 60000, Test: 20000\n",
      "\n",
      "2. Training Cox Model (Linear)...\n",
      "   -> Cox C-Index: 0.7395\n",
      "\n",
      "   Key Risk Factors (Hazard Ratios):\n",
      "covariate\n",
      "SATO             0.553880\n",
      "HIGH_LTV_FLAG    0.392062\n",
      "FICO_BUCKET      0.027650\n",
      "Name: coef, dtype: float64\n",
      "\n",
      "3. Training RSF (Non-Linear)...\n",
      "   -> RSF C-Index: 0.7163\n",
      "\n",
      "========================================\n",
      "DEFAULT MODEL COMPARISON\n",
      "========================================\n",
      "Cox Model: 0.7395\n",
      "RSF Model: 0.7163\n",
      "\n",
      "Winner: Cox Model\n",
      "Saving models to disk...\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for Survival Analysis\n",
    "# pip install lifelines scikit-survival\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "import joblib\n",
    "# --- CONFIGURATION ---\n",
    "DATA_FILE = DATA_DIR + \"default_modeling_dataset.csv\"\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Default Data...\")\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    \n",
    "    # Define Default-Specific Features\n",
    "    features = ['SATO', 'FICO_BUCKET', 'HIGH_LTV_FLAG', 'CREDIT_SCORE', \n",
    "                'ORIGINAL_LTV', 'ORIGINAL_DEBT_TO_INCOME_RATIO']\n",
    "    \n",
    "    target_duration = 'DURATION'\n",
    "    target_event = 'IS_DEFAULT' # 1=Default, 0=Censored/Prepaid\n",
    "    \n",
    "    # Drop Missing\n",
    "    df_clean = df[features + [target_duration, target_event]].dropna()\n",
    "    print(f\"   Modeling Universe: {len(df_clean)} loans\")\n",
    "    \n",
    "    # Split Data\n",
    "    train, test = train_test_split(df_clean, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "    print(f\"   Training: {len(train)}, Test: {len(test)}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # MODEL A: COX PROPORTIONAL HAZARD\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n2. Training Cox Model (Linear)...\")\n",
    "    cph_default = CoxPHFitter()\n",
    "    try:\n",
    "        cph_default.fit(train, duration_col=target_duration, event_col=target_event)\n",
    "        \n",
    "        # Evaluate\n",
    "        cox_pred = cph_default.predict_partial_hazard(test)\n",
    "        cox_c = concordance_index(test[target_duration], -cox_pred, test[target_event])\n",
    "        print(f\"   -> Cox C-Index: {cox_c:.4f}\")\n",
    "        \n",
    "        print(\"\\n   Key Risk Factors (Hazard Ratios):\")\n",
    "        print(cph_default.params_.sort_values(ascending=False).head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Cox Error: {e}\")\n",
    "        cox_c = 0\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # MODEL B: RANDOM SURVIVAL FOREST\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n3. Training RSF (Non-Linear)...\")\n",
    "    X_train = train[features]\n",
    "    y_train = Surv.from_dataframe(target_event, target_duration, train)\n",
    "    \n",
    "    X_test = test[features]\n",
    "    y_test = Surv.from_dataframe(target_event, target_duration, test)\n",
    "    \n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=100, \n",
    "        min_samples_leaf=10, # Allow smaller leaves to catch rare defaults\n",
    "        n_jobs=-1, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    rsf.fit(X_train, y_train)\n",
    "    \n",
    "    rsf_c = rsf.score(X_test, y_test)\n",
    "    print(f\"   -> RSF C-Index: {rsf_c:.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # COMPARISON\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"DEFAULT MODEL COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Cox Model: {cox_c:.4f}\")\n",
    "    print(f\"RSF Model: {rsf_c:.4f}\")\n",
    "    \n",
    "    if rsf_c > cox_c:\n",
    "        print(\"\\nWinner: Random Survival Forest\")\n",
    "        print(\"Reason: Default risk often has sharp 'cliffs' (e.g., FICO < 660) that trees handle better.\")\n",
    "    else:\n",
    "        print(\"\\nWinner: Cox Model\")\n",
    "\n",
    "\n",
    "\n",
    "    # ... (After fitting cph and rsf) ...\n",
    "\n",
    "    print(\"Saving models to disk...\")\n",
    "\n",
    "# 1. Save Cox Model\n",
    "# Note: Lifelines objects pickle well, but ensure you use the same library version when loading.\n",
    "    joblib.dump(cph_default, DATA_DIR + \"cox_default_model.pkl\")\n",
    "\n",
    "# 2. Save Random Survival Forest\n",
    "# Scikit-survival objects are scikit-learn compatible.\n",
    "    joblib.dump(rsf, DATA_DIR + \"rsf_default_model.pkl\")\n",
    "\n",
    "    print(\"Models saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90a40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
