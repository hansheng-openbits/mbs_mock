{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a394e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ================= USER CONFIGURATION =================\n",
    "#TARGET_YEAR = \"2018\"\n",
    "TARGET_YEARS = [2019, 2020]\n",
    "USERNAME = \"hansheng@openbits.io\"\n",
    "PASSWORD = \"[7vid|L|\"\n",
    "DOWNLOAD_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Downloads_zips/\"\n",
    "LOGIN_URL = \"https://freddiemac.embs.com/FLoan/secure/login.php?pagename=downloadA\"\n",
    "# ======================================================\n",
    "\n",
    "def setup_driver(download_path):\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path, exist_ok=True)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_path,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver(DOWNLOAD_DIR)\n",
    "\n",
    "    try:\n",
    "        # 1. Login Phase\n",
    "        driver.get(LOGIN_URL)\n",
    "        try:\n",
    "            driver.find_element(By.NAME, \"username\").send_keys(USERNAME)\n",
    "            driver.find_element(By.NAME, \"password\").send_keys(PASSWORD)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"\\n\" + \"!\"*50)\n",
    "        print(\"ACTION REQUIRED: Solve CAPTCHA and click LOGIN.\")\n",
    "        print(\"!\"*50 + \"\\n\")\n",
    "\n",
    "        # Wait up to 10 mins for the URL to indicate we are logged in\n",
    "        WebDriverWait(driver, 600).until(EC.url_contains(\"downloadA.php\"))\n",
    "        print(\">> Login URL detected. Checking for content...\")\n",
    "\n",
    "        # 2. TERMS OF USE CHECKER (The \"1 Link\" Fix)\n",
    "        # We wait up to 10 seconds to see if an \"Accept\" button exists\n",
    "        try:\n",
    "            accept_btn = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//input[@value='Accept' or @value='I Agree' or @value='Continue']\"))\n",
    "            )\n",
    "            print(\">> Terms of Use detected. Clicking 'Accept'...\")\n",
    "            accept_btn.click()\n",
    "            time.sleep(3) # Wait for reload\n",
    "        except:\n",
    "            print(\">> No 'Accept' button found. Proceeding...\")\n",
    "\n",
    "        # 3. WAIT FOR TABLE DATA\n",
    "        # This is the crucial fix: Don't scan until we see \"historical_data\"\n",
    "        print(\">> Waiting for file list to load...\")\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_element_located((By.PARTIAL_LINK_TEXT, \"historical_data\"))\n",
    "            )\n",
    "            print(\">> File list loaded successfully!\")\n",
    "        except:\n",
    "            print(\">> Warning: Timed out waiting for 'historical_data' links. Scanning page anyway...\")\n",
    "\n",
    "        # 4. FIND THE 2017 LINK\n",
    "        # We search specifically for the year in the link text\n",
    "\n",
    "        for year in TARGET_YEARS:\n",
    "            year_str = str(year)\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "            for link in links:\n",
    "                text = link.text\n",
    "                href = link.get_attribute(\"href\")\n",
    "            # Match if '2017' is in the text AND it's a zip file\n",
    "                if text and year_str in text and \"zip\" in text.lower():\n",
    "                    target_link = link\n",
    "                    break\n",
    "            # Fallback: check href if text is empty\n",
    "                if href and year_str in href and \".zip\" in href:\n",
    "                    target_link = link\n",
    "                    break\n",
    "\n",
    "            if target_link:\n",
    "                filename = target_link.text if target_link.text else target_link.get_attribute(\"href\").split(\"/\")[-1]\n",
    "                print(f\">> FOUND: {filename}\")\n",
    "                print(\">> Clicking download...\")\n",
    "            \n",
    "            # Scroll to it and click\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", target_link)\n",
    "                time.sleep(1)\n",
    "                target_link.click()\n",
    "\n",
    "\n",
    "            # 5. MONITOR DOWNLOAD\n",
    "            print(\">> Download started. Monitoring folder...\")\n",
    "            while True:\n",
    "                files = os.listdir(DOWNLOAD_DIR)\n",
    "                if any(f.endswith(\".crdownload\") for f in files):\n",
    "                    time.sleep(2)\n",
    "                elif any(year_str in f for f in files):\n",
    "                    print(f\">> SUCCESS: {year_str} file found in folder!\")\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "        else:\n",
    "            print(f\">> ERROR: No {year_str} zip file found on page.\")\n",
    "            print(\">> Debug: Here are the first 10 links I see:\")\n",
    "            for l in links[:10]:\n",
    "                print(f\"   - Text: {l.text} | Href: {l.get_attribute('href')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc80aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 top-level zip files.\n",
      ">> Extracting Main File: historical_data_2020.zip ...\n",
      ">> Extracting Main File: historical_data_2018.zip ...\n",
      ">> Extracting Main File: historical_data_2019.zip ...\n",
      ">> Extracting Main File: historical_data_2017.zip ...\n",
      "\n",
      ">> Found 16 internal zip files. Extracting deeper...\n",
      "   Converted: historical_data_2018Q1.zip\n",
      "   Converted: historical_data_2017Q4.zip\n",
      "   Converted: historical_data_2020Q1.zip\n",
      "   Converted: historical_data_2017Q3.zip\n",
      "   Converted: historical_data_2018Q2.zip\n",
      "   Converted: historical_data_2020Q4.zip\n",
      "   Converted: historical_data_2020Q2.zip\n",
      "   Converted: historical_data_2019Q3.zip\n",
      "   Converted: historical_data_2018Q3.zip\n",
      "   Converted: historical_data_2019Q2.zip\n",
      "   Converted: historical_data_2018Q4.zip\n",
      "   Converted: historical_data_2017Q1.zip\n",
      "   Converted: historical_data_2019Q4.zip\n",
      "   Converted: historical_data_2019Q1.zip\n",
      "   Converted: historical_data_2017Q2.zip\n",
      "   Converted: historical_data_2020Q3.zip\n",
      "\n",
      "==================================================\n",
      "DONE! All files extracted to lowest level.\n",
      "Location: /media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "# ================= USER CONFIGURATION =================\n",
    "# 1. SOURCE: Where are the zip files now?\n",
    "SOURCE_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Downloads_zips/\"\n",
    "\n",
    "# 2. TARGET: Where do you want the raw files?\n",
    "# (I added a new folder name \"Extracted_Data\" to keep it organized)\n",
    "TARGET_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\"\n",
    "# ======================================================\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except PermissionError:\n",
    "            print(f\"\\nCRITICAL ERROR: Permission Denied creating {directory}\")\n",
    "            print(\"You typically need to run 'sudo chown' on Linux external drives.\")\n",
    "            print(f\"Try running this in terminal: sudo chown -R $USER {os.path.dirname(directory)}\")\n",
    "            exit(1)\n",
    "\n",
    "def recursive_unzip(source, target):\n",
    "    ensure_dir(target)\n",
    "    \n",
    "    # --- PHASE 1: Extract Main Zips from Source to Target ---\n",
    "    zip_files = glob.glob(os.path.join(source, \"*.zip\"))\n",
    "    \n",
    "    if not zip_files:\n",
    "        print(f\"No .zip files found in {source}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(zip_files)} top-level zip files.\")\n",
    "    \n",
    "    for zip_path in zip_files:\n",
    "        filename = os.path.basename(zip_path)\n",
    "        print(f\">> Extracting Main File: {filename} ...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(target)\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"   [Error] Bad Zip File: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   [Error] Failed to extract {filename}: {e}\")\n",
    "\n",
    "    # --- PHASE 2: Handle Nested Zips (The \"Lowest Level\" part) ---\n",
    "    # We loop until no .zip files remain in the target folder\n",
    "    while True:\n",
    "        # Find zips inside the target folder (e.g., 2017Q1.zip extracted from 2017.zip)\n",
    "        nested_zips = []\n",
    "        for root, dirs, files in os.walk(target):\n",
    "            for file in files:\n",
    "                if file.endswith(\".zip\"):\n",
    "                    nested_zips.append(os.path.join(root, file))\n",
    "        \n",
    "        if not nested_zips:\n",
    "            break # No more zips found, we are done!\n",
    "\n",
    "        print(f\"\\n>> Found {len(nested_zips)} internal zip files. Extracting deeper...\")\n",
    "        \n",
    "        for nested_zip_path in nested_zips:\n",
    "            try:\n",
    "                # Extract contents to the same folder where the zip sits\n",
    "                parent_folder = os.path.dirname(nested_zip_path)\n",
    "                \n",
    "                with zipfile.ZipFile(nested_zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(parent_folder)\n",
    "                \n",
    "                # DELETE the intermediate zip to save space and clean up\n",
    "                os.remove(nested_zip_path)\n",
    "                print(f\"   Converted: {os.path.basename(nested_zip_path)}\")\n",
    "                \n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"   [Warning] Corrupt nested zip: {nested_zip_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   [Error] Could not process {nested_zip_path}: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DONE! All files extracted to lowest level.\")\n",
    "    print(f\"Location: {TARGET_DIR}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recursive_unzip(SOURCE_DIR, TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_columns = [\"Credit_Score\",\"First_Payment_Date\",\"First_Time_Home_Buyer\",\"Maturity_Date\",\"MSA\",\"Insurance_pct\",\"Units\",\"Occupancy\",\n",
    "                \"Combined_LTV\",\"DTI\",\"UPB\",\"LTV\",\"Interest\",\"Channel\",\"PPM\",\"Product_Type\",\"State\",\"Property_Type\",\"ZIP\",\"ID\",\"Loan_Purpose\",\n",
    "                \"Term\",\"Borrowers\",\"Seller\",\"Servicer\",\"Super_Confirming\",\"Program\"]\n",
    "\n",
    "perf_columns = [\"ID\",\"Date\",\"C_UPB\",\"Del_Status\",'Loan_Age',\"Remain_Age\",\"Repurchase\",\"Modification\",\"Zero_UPB\",\"Zero_UPB_Date\",\"C_IR\",\n",
    "                \"C_deferred_UPB\",\"DDLPI\",\"MI_Rec\",\"Net_Sales\",\"Non_MI_Rec\",\"Expenses\",\"Legal_Costs\",\"Maintain_Cost\",\"Taxes\",\"Other_Expenses\",\n",
    "                \"Actual_Loss\",\"Modification_Cost\",\"Step_Modification\",\"Deferred_PP\",\"ELTV\",\"Zero_UPB_Removal\",\"Del_Accrued_Interest\",\"Del_Disaster\",\"Borrower_Assistance\"]\n",
    "\n",
    "#historical_data_Q12017.txt is the origination data of mortgages originated in Q12017,  historical_data_time_Q12017.txt is the performance data, for 2017Q1 to 2020Q4 randomly sample 5000 mortgages and combine into one file \n",
    "# TARGET_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\" \n",
    "ORIG_COLUMNS = [\n",
    "    \"CREDIT_SCORE\", \"FIRST_PAYMENT_DATE\", \"FIRST_TIME_HOMEBUYER_FLAG\", \"MATURITY_DATE\",\n",
    "    \"MSA\", \"MORTGAGE_INSURANCE_PERCENTAGE\", \"NUMBER_OF_UNITS\", \"OCCUPANCY_STATUS\",\n",
    "    \"ORIGINAL_CLTV\", \"ORIGINAL_DEBT_TO_INCOME_RATIO\", \"ORIGINAL_UPB\", \"ORIGINAL_LTV\",\n",
    "    \"ORIGINAL_INTEREST_RATE\", \"CHANNEL\", \"PREPAYMENT_PENALTY_MORTGAGE_FLAG\",\n",
    "    \"AMORTIZATION_TYPE\", \"PROPERTY_STATE\", \"PROPERTY_TYPE\", \"POSTAL_CODE\",\n",
    "    \"LOAN_SEQUENCE_NUMBER\", \"LOAN_PURPOSE\", \"ORIGINAL_LOAN_TERM\", \"NUMBER_OF_BORROWERS\",\n",
    "    \"SELLER_NAME\", \"SERVICER_NAME\", \"SUPER_CONFORMING_FLAG\", \"PRE_HARP_LOAN_SEQUENCE_NUMBER\",\n",
    "    \"PROGRAM_INDICATOR\", \"HARP_INDICATOR\", \"PROPERTY_VALUATION_METHOD\", \"INTEREST_ONLY_INDICATOR\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60cd3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing in: /media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\n",
      "Processing historical_data_2017Q1.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2017Q2.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2017Q3.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2017Q4.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2018Q1.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2018Q2.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2018Q3.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2018Q4.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2019Q1.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2019Q2.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2019Q3.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2019Q4.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2020Q1.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2020Q2.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2020Q3.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical_data_2020Q4.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4235/1711330823.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating data...\n",
      "Saving combined file to /media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/combined_sampled_mortgages_2017_2020.csv...\n",
      "Done!\n",
      "Total mortgages sampled: 80000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ---------------- CONFIGURATION ---------------- #\n",
    "TARGET_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\"\n",
    "OUTPUT_FILE = os.path.join(TARGET_DIR, \"combined_sampled_mortgages_2017_2020.csv\")\n",
    "SAMPLE_SIZE = 5000\n",
    "YEARS = [2017, 2018, 2019, 2020]\n",
    "QUARTERS = [1, 2, 3, 4]\n",
    "\n",
    "# Freddie Mac Standard Origination Column Names (Common Schema)\n",
    "# If your files have headers, change header=None to header=0 in the read_csv call\n",
    "ORIG_COLUMNS = [\n",
    "    \"CREDIT_SCORE\", \"FIRST_PAYMENT_DATE\", \"FIRST_TIME_HOMEBUYER_FLAG\", \"MATURITY_DATE\",\n",
    "    \"MSA\", \"MORTGAGE_INSURANCE_PERCENTAGE\", \"NUMBER_OF_UNITS\", \"OCCUPANCY_STATUS\",\n",
    "    \"ORIGINAL_CLTV\", \"ORIGINAL_DEBT_TO_INCOME_RATIO\", \"ORIGINAL_UPB\", \"ORIGINAL_LTV\",\n",
    "    \"ORIGINAL_INTEREST_RATE\", \"CHANNEL\", \"PREPAYMENT_PENALTY_MORTGAGE_FLAG\",\n",
    "    \"AMORTIZATION_TYPE\", \"PROPERTY_STATE\", \"PROPERTY_TYPE\", \"POSTAL_CODE\",\n",
    "    \"LOAN_SEQUENCE_NUMBER\", \"LOAN_PURPOSE\", \"ORIGINAL_LOAN_TERM\", \"NUMBER_OF_BORROWERS\",\n",
    "    \"SELLER_NAME\", \"SERVICER_NAME\", \"SUPER_CONFORMING_FLAG\", \"PRE_HARP_LOAN_SEQUENCE_NUMBER\",\n",
    "    \"PROGRAM_INDICATOR\", \"HARP_INDICATOR\", \"PROPERTY_VALUATION_METHOD\", \"INTEREST_ONLY_INDICATOR\"\n",
    "]\n",
    "# ----------------------------------------------- #\n",
    "\n",
    "def main():\n",
    "    all_sampled_data = []\n",
    "\n",
    "    print(f\"Starting processing in: {TARGET_DIR}\")\n",
    "\n",
    "    for year in YEARS:\n",
    "        for q in QUARTERS:\n",
    "            # Construct the filename pattern based on your example: historical_data_Q12017.txt\n",
    "            file_suffix = f\"{year}Q{q}\"\n",
    "            filename = f\"historical_data_{file_suffix}.txt\"\n",
    "            file_path = os.path.join(TARGET_DIR, filename)\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Processing {filename}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Read the origination data\n",
    "                    # separator is usually pipe '|' for Freddie Mac, sometimes whitespace or comma.\n",
    "                    # Adjust sep='|' if your file is pipe-delimited.\n",
    "                    df = pd.read_csv(\n",
    "                        file_path, \n",
    "                        sep='|', \n",
    "                        header=None, \n",
    "                        names=ORIG_COLUMNS,\n",
    "                        index_col=False,\n",
    "                        low_memory=False\n",
    "                    )\n",
    "\n",
    "                    # Check if enough data exists to sample\n",
    "                    if len(df) >= SAMPLE_SIZE:\n",
    "                        sampled_df = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "                    else:\n",
    "                        print(f\"Warning: {filename} has fewer than {SAMPLE_SIZE} rows. Taking all.\")\n",
    "                        sampled_df = df\n",
    "\n",
    "                    # Add a column to track which quarter this came from (optional but helpful)\n",
    "                    sampled_df['SOURCE_QUARTER'] = file_suffix\n",
    "                    \n",
    "                    all_sampled_data.append(sampled_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filename}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {filename} - Skipping.\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if all_sampled_data:\n",
    "        print(\"Concatenating data...\")\n",
    "        final_df = pd.concat(all_sampled_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"Saving combined file to {OUTPUT_FILE}...\")\n",
    "        final_df.to_csv(OUTPUT_FILE, index=False, sep='|')\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        print(f\"Total mortgages sampled: {len(final_df)}\")\n",
    "    else:\n",
    "        print(\"No data was collected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fc4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup list from: /media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/combined_sampled_mortgages_2017_2020.csv\n",
      "Found 80000 mortgages across 16 quarters.\n",
      "Scanning historical_data_time_2017Q1.txt for 5000 Loan IDs...\n",
      "  -> Extracted 281985 performance records.\n",
      "Scanning historical_data_time_2017Q2.txt for 5000 Loan IDs...\n",
      "  -> Extracted 266333 performance records.\n",
      "Scanning historical_data_time_2017Q3.txt for 5000 Loan IDs...\n",
      "  -> Extracted 269531 performance records.\n",
      "Scanning historical_data_time_2017Q4.txt for 5000 Loan IDs...\n",
      "  -> Extracted 257471 performance records.\n",
      "Scanning historical_data_time_2018Q1.txt for 5000 Loan IDs...\n",
      "  -> Extracted 230767 performance records.\n",
      "Scanning historical_data_time_2018Q2.txt for 5000 Loan IDs...\n",
      "  -> Extracted 200383 performance records.\n",
      "Scanning historical_data_time_2018Q3.txt for 5000 Loan IDs...\n",
      "  -> Extracted 189793 performance records.\n",
      "Scanning historical_data_time_2018Q4.txt for 5000 Loan IDs...\n",
      "  -> Extracted 169667 performance records.\n",
      "Scanning historical_data_time_2019Q1.txt for 5000 Loan IDs...\n",
      "  -> Extracted 167166 performance records.\n",
      "Scanning historical_data_time_2019Q2.txt for 5000 Loan IDs...\n",
      "  -> Extracted 172664 performance records.\n",
      "Scanning historical_data_time_2019Q3.txt for 5000 Loan IDs...\n",
      "  -> Extracted 189185 performance records.\n",
      "Scanning historical_data_time_2019Q4.txt for 5000 Loan IDs...\n",
      "  -> Extracted 185692 performance records.\n",
      "Scanning historical_data_time_2020Q1.txt for 5000 Loan IDs...\n",
      "  -> Extracted 195176 performance records.\n",
      "Scanning historical_data_time_2020Q2.txt for 5000 Loan IDs...\n",
      "  -> Extracted 222642 performance records.\n",
      "Scanning historical_data_time_2020Q3.txt for 5000 Loan IDs...\n",
      "  -> Extracted 239341 performance records.\n",
      "Scanning historical_data_time_2020Q4.txt for 5000 Loan IDs...\n",
      "  -> Extracted 238834 performance records.\n",
      "\n",
      "Concatenating all performance history...\n",
      "Saving to /media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/combined_performance_history_2017_2020.csv...\n",
      "Done! Total rows: 3476630\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIGURATION ---------------- #\n",
    "TARGET_DIR = \"/media/hansheng/cc7df9bc-e728-4b8d-a215-b64f31876acc/cdo-tee-mock/prepayment/data/Extracted_data/\"\n",
    "INPUT_SAMPLED_FILE = os.path.join(TARGET_DIR, \"combined_sampled_mortgages_2017_2020.csv\")\n",
    "OUTPUT_PERFORMANCE_FILE = os.path.join(TARGET_DIR, \"combined_performance_history_2017_2020.csv\")\n",
    "\n",
    "CHUNK_SIZE = 100000  # Reads 100k rows at a time to prevent crashing\n",
    "\n",
    "# Standard Freddie Mac Performance Columns\n",
    "TIME_COLUMNS = [\n",
    "    \"LOAN_SEQUENCE_NUMBER\", \"MONTHLY_REPORTING_PERIOD\", \"CURRENT_ACTUAL_UPB\", \n",
    "    \"CURRENT_LOAN_DELINQUENCY_STATUS\", \"LOAN_AGE\", \"REMAINING_MONTHS_TO_LEGAL_MATURITY\", \n",
    "    \"REPURCHASE_FLAG\", \"MODIFICATION_FLAG\", \"ZERO_BALANCE_CODE\", \"ZERO_BALANCE_EFFECTIVE_DATE\", \n",
    "    \"CURRENT_INTEREST_RATE\", \"CURRENT_DEFERRED_UPB\", \"DUE_DATE_OF_LAST_PAID_INSTALLMENT\", \n",
    "    \"MI_RECOVERIES\", \"NET_SALES_PROCEEDS\", \"NON_MI_RECOVERIES\", \"EXPENSES\", \"LEGAL_COSTS\", \n",
    "    \"MAINTENANCE_AND_PRESERVATION_COSTS\", \"TAXES_AND_INSURANCE\", \"MISC_EXPENSES\", \n",
    "    \"ACTUAL_LOSS_CALCULATION\", \"MODIFICATION_COST\", \"STEP_MODIFICATION_FLAG\", \n",
    "    \"DEFERRED_PAYMENT_PLAN\", \"ESTIMATED_LOAN_TO_VALUE\", \"ZERO_BALANCE_REMOVAL_UPB\", \n",
    "    \"DELINQUENT_ACCRUED_INTEREST\", \"DELINQUENCY_DUE_TO_DISASTER\", \n",
    "    \"BORROWER_ASSISTANCE_STATUS_CODE\", \"CURRENT_MONTH_MODIFICATION_COST\", \"INTEREST_BEARING_UPB\"\n",
    "]\n",
    "# ----------------------------------------------- #\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading lookup list from: {INPUT_SAMPLED_FILE}\")\n",
    "    \n",
    "    # 1. Load the sampled origination data to get the IDs\n",
    "    try:\n",
    "        df_lookup = pd.read_csv(INPUT_SAMPLED_FILE, sep='|', low_memory=False, dtype={'LOAN_SEQUENCE_NUMBER': str})\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found. Please run the sampling script first.\")\n",
    "        return\n",
    "\n",
    "    # 2. Get unique Quarters to process\n",
    "    # We use the 'SOURCE_QUARTER' column we created in the previous step (e.g., \"Q12017\")\n",
    "    if 'SOURCE_QUARTER' not in df_lookup.columns:\n",
    "        print(\"Error: 'SOURCE_QUARTER' column missing. The script relies on this to find the correct time file.\")\n",
    "        return\n",
    "\n",
    "    unique_quarters = df_lookup['SOURCE_QUARTER'].unique()\n",
    "    print(f\"Found {len(df_lookup)} mortgages across {len(unique_quarters)} quarters.\")\n",
    "\n",
    "    all_performance_data = []\n",
    "\n",
    "    # 3. Iterate through each quarter present in the file\n",
    "    for quarter_id in unique_quarters:\n",
    "        \n",
    "        # Get the IDs specific to this quarter\n",
    "        quarter_subset = df_lookup[df_lookup['SOURCE_QUARTER'] == quarter_id]\n",
    "        target_ids = set(quarter_subset['LOAN_SEQUENCE_NUMBER'])\n",
    "        \n",
    "        # Construct the Time Filename\n",
    "        # NOTE: Adjust this pattern if your files are named \"2017Q1\" instead of \"Q12017\"\n",
    "        # Based on your prompt, we look for: historical_data_time_Q12017.txt\n",
    "        time_filename = f\"historical_data_time_{quarter_id}.txt\"\n",
    "        time_path = os.path.join(TARGET_DIR, time_filename)\n",
    "\n",
    "        if os.path.exists(time_path):\n",
    "            print(f\"Scanning {time_filename} for {len(target_ids)} Loan IDs...\")\n",
    "            \n",
    "            try:\n",
    "                # Read in chunks\n",
    "                chunk_iterator = pd.read_csv(\n",
    "                    time_path, \n",
    "                    sep='|', \n",
    "                    header=None, \n",
    "                    names=TIME_COLUMNS,\n",
    "                    index_col=False, \n",
    "                    dtype={'LOAN_SEQUENCE_NUMBER': str}, \n",
    "                    chunksize=CHUNK_SIZE, \n",
    "                    low_memory=False\n",
    "                )\n",
    "\n",
    "                rows_found = 0\n",
    "                for chunk in chunk_iterator:\n",
    "                    # Filter: Keep row ONLY if Loan ID is in our target list\n",
    "                    filtered_chunk = chunk[chunk['LOAN_SEQUENCE_NUMBER'].isin(target_ids)]\n",
    "                    \n",
    "                    if not filtered_chunk.empty:\n",
    "                        # Append source quarter just in case\n",
    "                        filtered_chunk = filtered_chunk.copy()\n",
    "                        filtered_chunk['SOURCE_QUARTER'] = quarter_id\n",
    "                        \n",
    "                        all_performance_data.append(filtered_chunk)\n",
    "                        rows_found += len(filtered_chunk)\n",
    "\n",
    "                print(f\"  -> Extracted {rows_found} performance records.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {time_filename}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Time file not found: {time_filename}\")\n",
    "\n",
    "    # 4. Combine and Save\n",
    "    if all_performance_data:\n",
    "        print(\"\\nConcatenating all performance history...\")\n",
    "        final_df = pd.concat(all_performance_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"Saving to {OUTPUT_PERFORMANCE_FILE}...\")\n",
    "        final_df.to_csv(OUTPUT_PERFORMANCE_FILE, index=False, sep='|')\n",
    "        print(f\"Done! Total rows: {len(final_df)}\")\n",
    "    else:\n",
    "        print(\"No performance data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0b499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
